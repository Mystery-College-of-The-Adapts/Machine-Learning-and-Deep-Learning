{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (ConvNets or CNNs)\n",
    "\n",
    "> **Convolutional Networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.** ~ Deep Learning Book\n",
    "\n",
    "\n",
    "# 1. Intuition\n",
    "Let's develop better intuition for how Convolutional Neural Networks (CNN) work. We'll examine how humans classify images, and then see how CNNs use similar approaches.\n",
    "\n",
    "Let’s say we wanted to classify the following image of a dog as a Golden Retriever.\n",
    "<img src=\"assets/golden.jpg\"  width=\"200\" height=\"40\" alt=\"\">\n",
    "\n",
    "As humans, how do we do this?\n",
    "\n",
    "One thing we do is that we identify certain parts of the dog, such as the nose, the eyes, and the fur. We essentially break up the image into smaller pieces, recognize the smaller pieces, and then combine those pieces to get an idea of the overall dog.\n",
    "\n",
    "In this case, we might break down the image into a combination of the following:\n",
    "\n",
    "- A nose\n",
    "- Two eyes\n",
    "- Golden fur\n",
    "\n",
    "\n",
    "These pieces can be seen below:\n",
    "<img src=\"assets/eye.png\"  width=\"200\" height=\"40\" alt=\"\">\n",
    "<img src=\"assets/nose.png\"  width=\"200\" height=\"40\" alt=\"\">\n",
    "<img src=\"assets/fur.png\"  width=\"200\" height=\"40\" alt=\"\">\n",
    "\n",
    "**Going One Step Further**\n",
    "\n",
    "But let’s take this one step further. How do we determine what exactly a nose is? A Golden Retriever nose can be seen as an oval with two black holes inside it. Thus, one way of classifying a Retriever’s nose is to to break it up into smaller pieces and look for black holes (nostrils) and curves that define an oval as shown below.\n",
    "<img src=\"assets/curve.png\"  width=\"200\" height=\"40\" alt=\"\">\n",
    "<img src=\"assets/nostril.png\"  width=\"200\" height=\"40\" alt=\"\">\n",
    "\n",
    "Broadly speaking, this is what a CNN learns to do. It learns to recognize basic lines and curves, then shapes and blobs, and then increasingly complex objects within the image. Finally, the CNN classifies the image by combining the larger, more complex objects.\n",
    "\n",
    "In our case, the levels in the hierarchy are:\n",
    "\n",
    "- Simple shapes, like ovals and dark circles\n",
    "- Complex objects (combinations of simple shapes), like eyes, nose, and fur\n",
    "- The dog as a whole (a combination of complex objects)\n",
    "\n",
    "With deep learning, we don't actually program the CNN to recognize these specific features. Rather, the CNN learns on its own to recognize such objects through forward propagation and backpropagation!\n",
    "\n",
    "It's amazing how well a CNN can learn to classify images, even though we never program the CNN with information about specific features to look for.\n",
    "![](assets/heirarchy-diagram.jpg)\n",
    "\n",
    "A CNN might have several layers, and each layer might capture a different level in the hierarchy of objects. The first layer is the lowest level in the hierarchy, where the CNN generally classifies small parts of the image into simple shapes like horizontal and vertical lines and simple blobs of colors. The subsequent layers tend to be higher levels in the hierarchy and generally classify more complex ideas like shapes (combinations of lines), and eventually full objects like dogs.\n",
    "\n",
    "Once again, the CNN **learns all of this on its own**. We don't ever have to tell the CNN to go looking for lines or curves or noses or fur. The CNN just learns from the training set and discovers which characteristics of a Golden Retriever are worth looking for.\n",
    "\n",
    "That's a good start! Hopefully you've developed some intuition about how CNNs work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Architecture Overview\n",
    "\n",
    "Convolutional Neural Networks (CNN) take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers a ConVNet have neurons arranged in 3 dimensions: **width, height, depth.**\n",
    "\n",
    "1) ![](assets/neural_net2.jpeg)\n",
    "<br><br>\n",
    "2) ![](assets/cnn.jpeg)\n",
    "\n",
    "> 1: A regular 3-layer Neural Network. 2: A ConvNet arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers. Every layer of a ConvNet transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels).\n",
    "\n",
    "\n",
    "# 3. Layers used to build ConvNets\n",
    "A simple ConvNet is a sequence of layers, and every layer of a ConvNet transforms one volume of activations to another through \n",
    "a differentiable function. \n",
    "\n",
    "Three main types of layers to build ConvNet architectures:\n",
    "- **Convolutional Layer**\n",
    "- **Pooling Layer**\n",
    "- **Fully-Connected Layer**\n",
    "\n",
    "*Example Architecture:*\n",
    "\n",
    "A simple ConvNet for [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) Classification could have the architecture \n",
    "**[INPUT - CONV - RELU - POOL - FC].**\n",
    "\n",
    "- INPUT[32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R, G, B.\n",
    "- CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters. (Filters are explained in later paragraph)\n",
    "- RELU layer will apply an elementwise activation function, such as the $max(0,x)$ thresholding at zero. This leaves the size of the volume unchanged([32x32x12]).\n",
    "- POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].\n",
    "- FC(full-connected) layer will compute the class scores, resulting in volume of size ([1x1x10]), where each of the 10 numbers correspond to a class score, such as among the 10 categories of  [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Notice that some layers contain parameters and other don't. In particular, the CONV/FC layers perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons).\n",
    "\n",
    "On the other hand, RELU/POOL layers will implement a fixed function. The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.\n",
    "\n",
    "In summary:\n",
    "\n",
    "\n",
    "- A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)\n",
    "- There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)\n",
    "- Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function\n",
    "- Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don’t)\n",
    "- Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn’t)\n",
    "\n",
    "![](assets/convnet.jpeg)\n",
    "\n",
    "# 5. Convolutional Layer\n",
    "The Convolutional layer is the core building block of a ConvNet that does most of the computational heavy lifting.\n",
    "\n",
    "The CONV layer's parameters consist of a set of learnable filters. Every filter \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 5. Filters\n",
    "The first step for a CNN is to break up the image into smaller pieces We do this by selecting width and height that defines a filter.\n",
    "\n",
    "The filter looks at small pieces, or patches, of the image. These patches are the same size as the filter.\n",
    "![](assets/filter.png)\n",
    "\n",
    "\n",
    "We then simply slide this filter horizontally or vertically to focus on a different piece of the image. \n",
    "\n",
    "The amount by which the filter is referred to as **stride**. The stride is a **hyperparameter** which you, the engineer, can tune. Increasing the stride reduces the size of your model by reducing the number of total patches each layer observes. However, this usually comes with a reduction in accuracy.\n",
    "\n",
    "Example:\n",
    "\n",
    "We first start with the patch outlined in red. The width and height of our filter define the size of this square.\n",
    "\n",
    "<img src=\"assets/dog.png\"  width=\"500\" height=\"400\" alt=\"\">\n",
    "\n",
    "\n",
    "We then move the square over to the right by a given stride (2 in this case) to get another patch.\n",
    "\n",
    "<img src=\"assets/dog2.png\"  width=\"500\" height=\"400\" alt=\"\">\n",
    "\n",
    "What is important here is that we are **grouping together adjacent pixels** and treating them as a collective.\n",
    "\n",
    "In a normal, non ConvNet, we would have ignored this adjacency. In a normal network, we would have connected every pixel in the input image to a neuron in the next layer. In doing so, we would not have taken advantage of the fact that pixels in an image are close together for a reason and have special meaning.\n",
    "\n",
    "By taking advantage of this local structure, our CNN learns to classify local patterns, like shapes and objects, in an image.\n",
    "\n",
    "### Filter Depth\n",
    "It's Common to have more than one filter. Different filters pick up different qualities of a patch. For example, one filter might look for a particular color, while another might look for a kind of object of a specific shape. The amount of filters in a ConvNet layer is called the **filter depth**\n",
    "![](assets/depth.png)\n",
    "In the above example, a patch is connected to a neuron in the next layer. Source: Michael Neilsen.\n",
    "\n",
    "\n",
    "**How many neurons does each patch connect to?**\n",
    "\n",
    "That's dependent on our filter depth. If we have a depth of **k**, we connect each patch of pixels to **k** neurons in the next layer. This gives us the height of **k** in the next layer, as shown below. In practice, **k** is a hyperparameter we tune, and most ConvNets tend to pick the same starting values.\n",
    "\n",
    "<img src=\"assets/filter-depth.png\"  width=\"300\" height=\"250\" alt=\"\">\n",
    "\n",
    "But why connect a single patch to multiple neurons in the next layer? Isn’t one neuron good enough?\n",
    "\n",
    "Multiple neurons can be useful because a patch can have multiple interesting characteristics that we want to capture.\n",
    "\n",
    "For example, one patch might include some white teeth, some blonde whiskers, and part of a red tongue. In that case, we might want a filter depth of at least three - one for each of teeth, whiskers, and tongue.\n",
    "\n",
    "![](assets/teeth-whiskers-tongue.png)\n",
    "\n",
    "This patch of the dog has many interesting features we may want to capture. These include the presence of teeth, the presence of whiskers, and the pink color of the tongue.\n",
    "Having multiple neurons for a given patch ensures that our ConvNet can learn to capture whatever characteristics the CNN learns are important.\n",
    "\n",
    "Remember that the ConvNet isn't \"programmed\" to look for certain characteristics. Rather, it learns on its own which characteristics to notice.\n",
    "\n",
    "**Some useful formulas to calculate feature map sizes** \n",
    "\n",
    "SAME padding equation:\n",
    "```python\n",
    "out_height = ceil(float(in_height) / float(strides[1]))\n",
    "out_width  = ceil(float(in_width) / float(strides[2]))\n",
    "```\n",
    "VALID padding equation:\n",
    "```python\n",
    "out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Parameters\n",
    "Convolution leverages three important ideas that can help improve a machine learning system:\n",
    "- Sparse interactions\n",
    "- Parameter sharing\n",
    "- Equivariant representations\n",
    "\n",
    "\n",
    "Traditional NN layers use matrix multiplication by a matrix parameters with a separate parameter describing the interaction between each input unit and each output unit. CNN, however, typically have **sparse interactions**. This is accomplished by making the kernel smaller than the input. For example, when processing an image the input image might have thousands or millions of pixels, but we can detect small, meaningful features such as edges with kernels that occupy only tens or hundreds of pixels.\n",
    "\n",
    "Therefore, we can store fewer parameters and reduce memory requirements of the model.\n",
    "\n",
    "### Parameter Sharing\n",
    "<img src=\"assets/kitten.png\"  width=\"400\" height=\"300\" alt=\"\">\n",
    "\n",
    "\n",
    "**Parameter sharing** refers to using the same parameter for more than one function in a model. In a CNN, each member of the kernels is used at every position of the input(except perhaps some of the boundary pixels, depending on the design decisions regarding the boundary). This reduces the storage requirements of the model to $k$ parameters. \n",
    "\n",
    "\n",
    "In a CNN, the particular form of parameter sharing causes the layer to have a property called **equivariance** to translation. A function is equivariant when the input changes, the output changes the same way. For Example a picture with a kitten in the middle. For a CNN it doesn't matter if the CNN is fed with a picture with a kitten in the corner. It will classify the same kitten as a kitten.\n",
    "\n",
    "When we are trying to classify a picture of a cat, we don’t care where in the image a cat is. If it’s in the top left or the bottom right, it’s still a cat in our eyes. We would like our CNNs to also possess this ability known as translation invariance. How can we achieve this?\n",
    "\n",
    "\n",
    "If we want a cat that’s in the top left patch to be classified in the same way as a cat in the bottom right patch, we need the weights and biases corresponding to those patches to be the same, so that they are classified the same way.\n",
    "\n",
    "This is exactly what we do in CNNs. The weights and biases we learn for a given output layer are shared across all patches in a given input layer. Note that as we increase the depth of our filter, the number of weights and biases we have to learn still increases, as the weights aren't shared across the output channels.\n",
    "\n",
    "There’s an additional benefit to sharing our parameters. If we did not reuse the same weights across all patches, we would have to learn new parameters for every single patch and hidden layer neuron pair. This does not scale well, especially for higher fidelity images. Thus, sharing parameters not only helps us with translation invariance, but also gives us a smaller, more scalable model.\n",
    "\n",
    "**Note:**\n",
    "- Convolution is not naturally equivariant to some other transformations, such as changes in the scale or rotation of an image.\n",
    "\n",
    "### Padding\n",
    "\n",
    "**A 5x5 grid with a 3x3 filter**\n",
    "![](assets/padding.png)\n",
    "\n",
    "\n",
    "Image Courtesy: Andrej Karpathy.\n",
    "\n",
    "Let's say we have a 5x5 grid and a filter of size 3x3 with a stride of 1. What's the width and height of the next layer? We see that we can fit at most three patches in each direction, giving us a dimension of 3x3 in our next layer. As we can see, the width and height of each subsequent layer decreases in such a scheme.\n",
    "\n",
    "\n",
    "In an ideal world, we'd be able to maintain the same width and height across layers so that we can continue to add layers without worrying about the dimensionality shrinking and so that we have consistency. How might we achieve this? One way is to simple add a border of 0s to our original 5x5 image. You can see what this looks like in the below image. \n",
    "Below is the same grid with 0 padding:\n",
    "\n",
    "![](assets/padding2.png)\n",
    "\n",
    "Image Courtesy: Andrej Karpathy.\n",
    "\n",
    "\n",
    "This would expand our original image to a 7x7. With this, we now see how our next layer's size is again a 5x5, keeping our dimensionality consistent.\n",
    "\n",
    "### Dimensionality\n",
    "From what we've learned so far, how can we calculate the number of neurons of each layer in our CNN?\n",
    "\n",
    "Given our input layer has a volume of W, our filter has a volume (height * width * depth) of F, we have a stride of S, and a padding of P, the following formula gives us the volume of the next layer: (W−F+2P)/S+1.\n",
    "\n",
    "Knowing the dimensionality of each additional layer helps us understand how large our model is and how our decisions around filter size and stride affect the size of our network.\n",
    "\n",
    "\n",
    "**1. Convolutional Layer Output Shape**\n",
    "\n",
    "*Introduction*\n",
    "\n",
    "Understanding dimensions will help you make accurate tradeoffs between model size and performance. As you'll see, some parameters have a much bigger impact on model size than others.\n",
    "\n",
    "*Setup*\n",
    "\n",
    "H = height, W = width, D = depth\n",
    "\n",
    "- We have an input of shape 32x32x3 (HxWxD)\n",
    "- 20 filters of shape 8x8x3 (HxWxD)\n",
    "- A stride of 2 for both the height and width (S)\n",
    "- Valid padding of size 1 (P)\n",
    "- Recall the formula for calculating the new height or width:\n",
    "\n",
    "```python \n",
    "new_height = (input_height - filter_height + 2 * P)/S + 1\n",
    "new_width = (input_width - filter_width + 2 * P)/S + 1\n",
    "```\n",
    "\n",
    "What's the shape of the output?\n",
    "\n",
    "We can get the new height and width with the above formula resulting in:\n",
    "\n",
    "(32 - 8 + 2 * 1)/2 + 1 = 14\n",
    "\n",
    "(32 - 8 + 2 * 1)/2 + 1 = 14\n",
    "\n",
    "The new depth is equal to the number of filters, which is 20.\n",
    "\n",
    "This would correspond to the following implementation in TensorFlow\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "input = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "\n",
    "# (height, width, input_depth, output_depth)\n",
    "filter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20)))\n",
    "\n",
    "filter_bias = tf.Variable(tf.Zeros(20))\n",
    "strides = [1, 2, 2, 1] #(batch, height, width, depth)\n",
    "padding = 'VALID'\n",
    "conv = tf.nn.Conv2d(input, filter_weights, strides, padding) + filter_bias\n",
    "```\n",
    "\n",
    "Note:\n",
    "\n",
    "Note the output shape of conv will be [1, 13, 13, 20]. It's 4D to account for batch size, but more importantly, it's not [1, 14, 14, 20]. This is because the padding algorithm TensorFlow uses is not exactly the same as the one above. An alternative algorithm is to switch padding from 'VALID' to SAME which would result in an output shape of [1, 16, 16, 20]. If you're curious how padding works in TensorFlow, \n",
    "read this [document](https://www.tensorflow.org/api_docs/python/tf/nn/convolution).\n",
    "\n",
    "\n",
    "**2. Number of Parameters**\n",
    "\n",
    "We're now going to calculate the number of parameters of the convolutional layer. The answer from the last quiz will come into play here!\n",
    "\n",
    "Being able to calculate the number of parameters in a neural network is useful since we want to have control over how much memory a neural network uses.\n",
    "\n",
    "*Setup*\n",
    "\n",
    "H = height, W = width, D = depth\n",
    "\n",
    "- We have an input of shape 32x32x3 (HxWxD)\n",
    "- 20 filters of shape 8x8x3 (HxWxD)\n",
    "- A stride of 2 for both the height and width (S)\n",
    "- Zero padding of size 1 (P)\n",
    "\n",
    "*Output Layer*\n",
    "\n",
    "- 14x14x20 (HxWxD)\n",
    "\n",
    "*Hint*\n",
    "\n",
    "- Without parameter sharing, each neuron in the output layer must connect to each neuron in the filter. In addition, each neuron in the output layer must also connect to a single bias neuron.\n",
    "- Without weight sharing every parameter in the filter has a connection with every neuron in the output. So, what we need to do is calculate the total number of parameters in the filter and the total number of neurons in the output.\n",
    "\n",
    "**Convolution Layer Parameters 1**\n",
    "\n",
    "How many parameters does the convolutional layer have (without parameter sharing)?\n",
    "\n",
    "\n",
    "Solution:\n",
    "\n",
    "There are 756560 total parameters. That's a HUGE amount! Here's how we calculate it:\n",
    "\n",
    "(8 x 8 x 3 + 1) x (14 x 14 x 20) = 756560 \n",
    "\n",
    "8 x 8 x 3 is the number of weights, we add 1 for the bias. Remember, each weight is assigned to every single part of the output (14 x 14 x 20). So we multiply these two numbers together and we get the final answer.\n",
    "\n",
    "\n",
    "\n",
    "**3. Parameter Sharing**\n",
    "\n",
    "Now we'd like you to calculate the number of parameters in the convolutional layer, if every neuron in the output layer shares its parameters with every other neuron in its same channel.\n",
    "\n",
    "This is the number of parameters actually used in a convolution layer ```python (tf.nn.conv2d())```\n",
    "\n",
    "*Setup*\n",
    "\n",
    "H = height, W = width, D = depth\n",
    "\n",
    "- We have an input of shape 32x32x3 (HxWxD)\n",
    "- 20 filters of shape 8x8x3 (HxWxD)\n",
    "- A stride of 2 for both the height and width (S)\n",
    "- Zero padding of size 1 (P)\n",
    "\n",
    "*Output Layer*\n",
    "\n",
    "- 14x14x20 (HxWxD)\n",
    "\n",
    "*Hint*\n",
    "\n",
    "- With parameter sharing, each neuron in an output channel shares its weights with every other neuron in that channel. So the number of parameters is equal to the number of neurons in the filter, plus a bias neuron, all multiplied by the number of channels in the output layer.\n",
    "\n",
    "**Convolution Layer Parameters 2**\n",
    "\n",
    "How many parameters does the convolution layer have (with parameter sharing)?\n",
    "\n",
    "Solution\n",
    "\n",
    "There are 3860 total parameters. That's 196 times fewer parameters! Here's how the answer is calculated:\n",
    "\n",
    "(8 x 8 x 3 + 1) x 20 = 3840 + 20 = 3860\n",
    "\n",
    "That's 3840 weights and 20 biases. This should look similar to the answer from the previous problem. The difference being it's just 20 instead of (14 x 14 x 20). Remember, with weight sharing we use the same filter for an entire depth slice. Because of this we can get rid of 14 x 14 and be left with only 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Visualizing ConvNets\n",
    "\n",
    "Let's look at an example ConvNets to see how they work in action.\n",
    "\n",
    "The CNN we will look at is trained on ImageNet as described in [this paper by Zeiler and Fergus](http://www.matthewzeiler.com/pubs/arxive2013/eccv2014.pdf). In the images below (from the same paper), we’ll see what each layer in this network detects and see how each layer detects more and more complex ideas.\n",
    "\n",
    "**Layer 1** \n",
    "![](assets/layer-1-grid.png)\n",
    "\n",
    "The images above are from [Matthew Zeiler and Rob Fergus' deep visualization toolbox](https://www.youtube.com/watch?v=ghEmQSxT6tw), which lets us visualize what each layer in a CNN focuses on.\n",
    "\n",
    "Each image in the above grid represents a pattern that causes the neurons in the first layer to activate - in other words, they are patterns that the first layer recognizes. The top left image shows a -45 degree line, while the middle top square shows a +45 degree line. \n",
    "\n",
    "Let's now see some example images that cause such activations. The below grid of images all activated the -45 degree line. Notice how they are all selected despite the fact that they have different colors, gradients, and patterns.\n",
    "![](assets/grid-layer-1.png)\n",
    "\n",
    "So, the first layer of our CNN clearly picks out very simple shapes and patterns like lines and blobs.\n",
    "\n",
    "\n",
    "**Layer 2** \n",
    "![](assets/layer2.png)\n",
    "A visualization of the second layer in the CNN. Notice how we are picking up more complex ideas like circles and stripes. The gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right.\n",
    "\n",
    "We'll skip layer 4, which continues this progression, and jump right to the fifth and final layer of this CNN.\n",
    "\n",
    "The last layer picks out the highest order ideas that we care about for classification, like dog faces, bird faces, and bicycles.\n",
    "\n",
    "On to TensorFlow\n",
    "This concludes our high-level discussion of Convolutional Neural Networks.\n",
    "\n",
    "Next we'll practice actually building these networks in TensorFlow.\n",
    "\n",
    "\n",
    "# 8. TensorFlow Convolution Layer\n",
    "\n",
    "Let's examine how to implemement a CNN in TensorFlow.\n",
    "\n",
    "Tensorflow provides the ```python tf.nn.conv2d() and tf.nn.bias_add() ``` functions to create our own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output depth\n",
    "k_output = 64\n",
    "\n",
    "# Image Properties\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 3\n",
    "\n",
    "# Convolution filter\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "\n",
    "# Input/Image\n",
    "input = tf.placeholder(tf.float32, shape=[None, image_width, image_height, color_channels])\n",
    "\n",
    "# Weight and bias\n",
    "weight = tf.Variable(tf.truncated_normal([filter_size_width, filter_size_height, color_channels, k_output]))\n",
    "\n",
    "bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "\n",
    "# Apply Convolution\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Add bias\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "\n",
    "# Apply activation function \n",
    "conv_layer = tf.nn.relu(conv_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. TensorFlow Pooling\n",
    "![](assets/pooling.jpg)\n",
    "\n",
    "A typical layer of a convolutional network consists of three stages. \n",
    "- In the first layer, the layer performs several convolutions in parallel to produce a set of linear activations. \n",
    "- In the second layer, each linear activation is run through a non linear activation function, such as the rectified linear activation function. This stage is sometimes called **the detector stage**. \n",
    "- In the third stage, we use a **pooling function** to modify the output of the layer further.\n",
    "\n",
    "A pooling function replaces the output of the net at a certain location with a summary statistic of the nearby outputs. For example, the *max pooling* operation reports the maximum output within a rectangular neighborhood. \n",
    "\n",
    "Pooling helps to make the representation become approximately *invariant* to small translations of the input. Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change.\n",
    "\n",
    "Because pooling summarizes the responses over a whole neighborhood, it is possible to use fewer pooling units than detector units, by reporting summary statistics for pooling regions spaced $k$ pixels rather than $1$ pixel apart\n",
    "\n",
    "![](assets/max-pooling.png)\n",
    "\n",
    "The image above is an example of **max pooling** with a 2x2 filter and stride of 2. The four 2x2 colors represent each time the filter was applied to find th maximum value. \n",
    "\n",
    "For example, [[1, 0], [4, 6]] becomes 6, because 6 is the maximum value in this set. Similarly, [[2, 3], [6, 8]] becomes 8.\n",
    "\n",
    "Conceptually, the benefit of the max pooling operation is to reduce the size of the input, and allow the neural network to focus on only the most important elements. Max pooling does this by only retaining the maximum value for each filtered area, and removing the remaining values.\n",
    "\n",
    "TensorFlow provides the *tf.nn.max_pool()* function to apply max pooling to your convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply  Max Pooling\n",
    "conv_layer = tf.nn.max_pool(conv_layer, \n",
    "                           ksize=[1,2,2,1],\n",
    "                           strides=[1,2,2,1],\n",
    "                           padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *tf.nn.max_pool()* function performs max pooling with the ksize parameter as the size of the filter and the strides parameter as the length of the stride. 2x2 filters with a stride of 2x2 are common in practice.\n",
    "\n",
    "The ksize and strides parameters are structured as 4-element lists, with each element corresponding to a dimension of the input tensor ([batch, height, width, channels]). For both ksize and strides, the batch and channel dimensions are typically set to 1.\n",
    "\n",
    "\n",
    "**Note:**\n",
    "\n",
    "A pooling layer is generally used to decrease the size of the output and prevent overfitting. Reducing overfitting is a consequence of the reducing the output size, which in turn, reduces the number of parameters in future layers.\n",
    "\n",
    "\n",
    "Recently, pooling layers have fallen out of favor. Some reasons are:\n",
    "\n",
    "- Recent datasets are so big and complex we're more concerned about underfitting.\n",
    "- Dropout is a much better regularizer.\n",
    "- Pooling results in a loss of information. Think about the max pooling operation as an example. We only keep the largest of n numbers, thereby disregarding n-1 numbers completely.\n",
    "\n",
    "\n",
    "\n",
    "# 10. Convolutional Network in TensorFlow\n",
    "\n",
    "It's time to walk through an example ConvNet in TensorFlow. \n",
    "\n",
    "The structure of this network follows the classic structure of CNN, which is a mix of convolutional layers and max pooling, followed by fully-connected layers.\n",
    "\n",
    "We are going to use the MNIST dataset. Let's import the MNIST dataset and using a convenient TensorFlow function to batch, scale, and One-Hot encode the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Number of samples to calculate validation and accuracy\n",
    "# Decrease this if you are running out of memory to calculate accuracy\n",
    "test_valid_size = 256\n",
    "\n",
    "# Network Parameters\n",
    "n_classes = 10   # MNINST total classes (0-9 digits)\n",
    "dropout = 0.75  # Dropout, probability to keep units\n",
    "\n",
    "\n",
    "# Weights and Biases\n",
    "\n",
    "# Store layers wieght & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutions**\n",
    "![](assets/convolution-schematic.gif)\n",
    "\n",
    "Image courtesy of [UFLDL Tutorial](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution)\n",
    "\n",
    "The above is an example of a convolution with a 3x3 filter and a stride of 1 being applied to data with a range of 0 to 1. The convolution for each 3x3 section is calculated against the weight, [[1, 0, 1], [0, 1, 0], [1, 0, 1]], then a bias is added to create the convolved feature on the right. In this case, the bias is zero. In TensorFlow, this is all done using *tf.nn.conv2d()* and *tf.nn.bias_add()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.nn.conv2d() function computes the convolution against weight W as shown above.\n",
    "\n",
    "In TensorFlow, stride is an array of 4 elements; the first element in the stride array indicates the stride for batch and last element indicates stride for features. It's good practice to remove the batches or features you want to skip from the dataset than to use stride. You can always set the first and last element to 1 in order to use all batches and features.\n",
    "\n",
    "The middle two elements are the strides for height and width respectively. I've mentioned stride as one number because you usually have a square stride where height = width. When someone says they are using a stride of 3, they usually mean tf.nn.conv2d(x, W, strides=[1, 3, 3, 1]).\n",
    "\n",
    "To make life easier, the code is using tf.nn.bias_add() to add the bias. Using tf.add() doesn't work when the tensors aren't the same shape.\n",
    "\n",
    "**Max Pooling**\n",
    "\n",
    "![](assets/maxpool.jpeg)\n",
    "\n",
    "The above is an example of max pooling with a 2x2 filter and stride of 2. The left square is the input and the right square is the output. The four 2x2 colors in input represents each time the filter was applied to create the max on the right side. For example, [[1, 1], [5, 6]] becomes 6 and [[3, 2], [1, 2]] becomes 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.nn.max_pool() function does exactly what you would expect, it performs max pooling with the ksize parameter as the size of the filter.\n",
    "\n",
    "**Model**\n",
    "\n",
    "![](assets/arch.png)\n",
    "\n",
    "In the code below, we're creating 3 layers alternating between convolutions and max pooling followed by a fully connected and output layer. The transformation of each layer to new dimensions are shown in the comments. For example, the first layer shapes the images from 28x28x1 to 28x28x32 in the convolution step. Then next step applies max pooling, turning each sample into 14x14x32. All the layers are applied from conv1 to output, producing 10 class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    \n",
    "    # Layer 1 - 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    \n",
    "    # Layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # Fully connected layer - 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    \n",
    "    # Output Layer - Class prediciton - 1024 to 10\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            \n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                    x: batch_x,\n",
    "                    y: batch_y,\n",
    "                    keep_prob: 1.\n",
    "                })\n",
    "            \n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                    x: batch_x,\n",
    "                    y: batch_y, \n",
    "                    keep_prob: 1.\n",
    "                })\n",
    "            \n",
    "            valid_acc = sess.run(accuracy, feed_dict={\n",
    "                    x: mnist.validation.images[:test_valid_size],\n",
    "                    y: mnist.validation.labels[:test_valid_size],\n",
    "                    keep_prob: 1.\n",
    "                })\n",
    "            \n",
    "            print('Epoch {:>2}, Batch {:>3} - Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "                valid_acc))\n",
    "            \n",
    "            # Calculate Test Accuracy\n",
    "            test_acc = sess.run(accuracy, feed_dict={\n",
    "                    x: mnist.test.images[:test_valid_size],\n",
    "                    y: mnist.test.labels[:test_valid_size],\n",
    "                    keep_prob: 1.\n",
    "                })\n",
    "            \n",
    "            print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "- [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/index.html)\n",
    "- [Goodfellow-et-al-2016 - Deep Learning](http://www.deeplearningbook.org/)\n",
    "- [Udacity](https://www.udacity.com/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
