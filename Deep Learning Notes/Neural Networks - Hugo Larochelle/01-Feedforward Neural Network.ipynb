{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics:\n",
    "--------\n",
    "\n",
    "- Connection weights\n",
    "- bias\n",
    "- activation function\n",
    "- capacity\n",
    "- decision boundary of neuron\n",
    "- single hidden layer neural network\n",
    "- softmax activation function\n",
    "- multilayer neural network\n",
    "- Universal approximation theorem\n",
    "\n",
    "\n",
    "\n",
    "_______________________________________\n",
    "______________________________________\n",
    "\n",
    "## Artificial Neuron\n",
    "\n",
    "- **Neuron pre-activation(or input activation)**\n",
    "$$a(\\textbf{x})= b + \\sum _{i}w_i x_i = b + \\textbf{W}^T\\textbf{x}$$\n",
    "\n",
    "- **Neuron (output) activation**\n",
    "$$h(\\textbf{x})= g (a(\\textbf{x})) = g(b + \\sum _{i}w_i x_i)$$\n",
    "![](assets/1.png)\n",
    "- $\\textbf{W}$ are the connection weights\n",
    "- $b$ is the neuron bias\n",
    "- $g(.)$ is called the activation function\n",
    "\n",
    "![](assets/2.png)\n",
    "\n",
    "## Activation Function\n",
    "- **Linear activation function** \n",
    "$$g(a)=a$$\n",
    "![](assets/3.png)\n",
    " - Performs no input squashing\n",
    " - Not very interesting\n",
    "____________________________\n",
    "\n",
    "- **Sigmoid activation function** \n",
    "$$g(a)=sigma(a)=\\frac{1}{1+e^{-a}}$$\n",
    "![](assets/4.png)\n",
    " - Squashes the neuron's pre-activation between 0 and 1.\n",
    " - Always Positive\n",
    " - Bounded\n",
    " - Strictly increasing\n",
    "_______________________________________\n",
    "\n",
    "- **Hyperbolic tangent ($\\tanh $) activation function** \n",
    "$$g(a)= \\tanh(a) = \\frac{e^{(a)}-e^{(-a)}}{e^{(a)}+e^{(-a)}}= \\frac{e^{(2a)}-1}{e^{(2a)}+1}$$\n",
    "![](assets/5.png)\n",
    " - Squashes the neuron's pre activation between -1 and 1\n",
    " - Can be positive or negative\n",
    " - Bounded\n",
    " - Strictly increasing\n",
    " \n",
    "_________________________________\n",
    "\n",
    "- **Rectified linear activation function** \n",
    "$$g(a)= reclin(a) = max(0,a)$$\n",
    "![](assets/6.png)\n",
    " - Bounded below by 0 (always non negative)\n",
    " - Not upper bounded\n",
    " - Strictly increasing\n",
    " - Tends to give neurons with sparse activities\n",
    " \n",
    " \n",
    " \n",
    "## Capacity of single neuron\n",
    "\n",
    "- Could do binary classification:\n",
    " - with sigmoid and tanh, can interpret neuron as estimating $p(y=1\\mid \\textrm{x})$\n",
    " - also known as logistic regression classifier\n",
    " - if greater than 0.5, predict class 1\n",
    " - otherwise, predict class 0\n",
    " ![](assets/7.png)\n",
    " \n",
    "- Can solve linearly separable problems\n",
    "![](assets/8.png)\n",
    "\n",
    "- Can't solve non linear separable problems\n",
    "![](assets/9.png)\n",
    "- Unless the input is transformed in a better representation\n",
    "\n",
    "## Multilayer neural network\n",
    "\n",
    "1. single hidden layer neural network \n",
    "--------------------------------\n",
    "\n",
    "\n",
    "![](assets/10.png)\n",
    "- Hidden layer pre-activation:\n",
    "$$a(\\textbf{x})= b^{(1)}+ \\textbf{W}^{(1)}\\textbf{x}$$\n",
    "$$\\begin{pmatrix}\n",
    "a(\\textbf{x})_i= b^{(1)}_i +\\sum _j \\textbf{W}^{(1)}_{i,j}\\textbf{x}_j\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "- Hidden layer activation:\n",
    "$$h(\\textrm{x})=g(a(\\textrm{x}))$$\n",
    "\n",
    "- Output layer activation:\n",
    "\n",
    "$$f( \\textbf{x})= o(b^{(2)} + \\textbf{w}^{(a)^T} \\textbf{h}^{(1)} \\textbf{x})$$\n",
    "\n",
    "o: is the output activation function\n",
    "\n",
    "2. softmax activation function\n",
    "---------------------------\n",
    "\n",
    "- For multi-class classification:\n",
    " - we need multi outputs ( 1 output per class)\n",
    " - we would like to estimate the conditional probability  $p(y=c\\mid \\textrm{x})$\n",
    " \n",
    "- we use the softmax activation function at the output:\n",
    "$$\\textbf{0(a)} = softmax(\\textbf{a})= \\begin{bmatrix}\n",
    "\\frac{e^{(a_1)}}{\\sum_c e^{(a_c)} } &  \n",
    "\\cdots  & \n",
    "\\frac{e^{(a_C)}}{\\sum_c e^{(a_c)} }\n",
    "\\end{bmatrix}^T$$\n",
    " - strictly positive\n",
    " - sums to one\n",
    " \n",
    "- Predicted class is the one with highest estimated probability\n",
    "\n",
    "\n",
    "3. Multilayer neural network\n",
    "---------------------------------\n",
    "\n",
    "- Could have $L$ hidden layers:\n",
    "![](assets/11.png)\n",
    " - layer pre-activation for $k>0$ $(\\textbf{h}^{(0)}(\\textbf{x})= \\textbf{x})$\n",
    " $$\\textbf{a}^{(k)}(\\textbf{x})= \\textbf{b}^{(k)}+\\textbf{W}^{(k)}\\textbf{h}^{(k-1)}(\\textbf{x})$$\n",
    " \n",
    " - hidden layer activation (k from 1 to L):\n",
    " $$\\textbf{h}(\\textbf{x})=g(\\textbf{a}^{(k)}(\\textbf{x}))$$\n",
    " \n",
    " - output layer activation (k=L+1):\n",
    " $$\\textbf{h}^{(L+1)}(\\textbf{x})=o(\\textbf{a}^{L+1}(\\textbf{x}))=f(\\textbf{x})$$\n",
    " \n",
    " ## Capacity of single hidden layer neural networks\n",
    " ![](assets/12.png)\n",
    " ![](assets/13.png)\n",
    " - **Universal approximation theorem**(Hornik, 1991)\n",
    "  > \"a single hidden layer neural network with a linear output unit can approximate any continuous function arbitrarily well, given enough hidden units\"\n",
    "  \n",
    " - The result applies for sigmoid, tanh, and many other hidden layer activation functions\n",
    " - This is a good result, but it doesn't mean there is a learning algorithm that can find the necessary parameter values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow3]",
   "language": "python",
   "name": "conda-env-tensorflow3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
