{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics:\n",
    "---------\n",
    "- Empirical risk minimization\n",
    "- regularization\n",
    "- Stochastic gradient descent(SGD)\n",
    "- Loss gradient at output\n",
    "\n",
    "In this notebook, we will look at the main principle behing training neural networks which is known as empirical risk minimization. we have seen what a multilayer neural network is:\n",
    "\n",
    "- Could have $L$ hidden layers:\n",
    "![](assets/11.png)\n",
    " - layer pre-activation for $k>0$ $(\\textbf{h}^{(0)}(\\textbf{x})= \\textbf{x})$\n",
    " $$\\textbf{a}^{(k)}(\\textbf{x})= \\textbf{b}^{(k)}+\\textbf{W}^{(k)}\\textbf{h}^{(k-1)}(\\textbf{x})$$\n",
    " \n",
    " - hidden layer activation (k from 1 to L):\n",
    " $$\\textbf{h}(\\textbf{x})=g(\\textbf{a}^{(k)}(\\textbf{x}))$$\n",
    " \n",
    " - output layer activation (k=L+1):\n",
    " $$\\textbf{h}^{(L+1)}(\\textbf{x})=o(\\textbf{a}^{L+1}(\\textbf{x}))=f(\\textbf{x})$$\n",
    " \n",
    " \n",
    "We haven't talked about how we can actually find good values for our parameters such that the neural network will have the correct behavior and will solve the problem we are trying to solve.\n",
    "\n",
    "\n",
    "1. Empirical risk minimization or structural rik minimization (when using regularization)\n",
    "=============================\n",
    "\n",
    "- framework to design learning algorithms\n",
    "$$\\arg \\min_{\\theta}\\frac{1}{T}\\sum_t l(f(\\textbf{x}^{(t)};\\theta),y^{(t)})+\\lambda\\Omega(\\theta)$$\n",
    "\n",
    " - $\\theta$ is the set of all parameters in a given model \n",
    "\n",
    "- $l(f(\\textbf{x}^{(t)};\\theta),y^{(t)})$, is a loss function \n",
    "- $\\Omega(\\theta)$ is a regularizer (penalizes certain values of $\\theta$ )\n",
    "- Learning is cast as optimization\n",
    " - Ideally, we'd optimize classification error, but it's not smooth\n",
    " - loss function is a surrogate for what we truly should optimization (e.g: upper bound)\n",
    " \n",
    " **Stochastic gradient descent (SGD)**\n",
    " ![](assets/14.png)\n",
    " \n",
    " \n",
    "2. Loss function \n",
    "====================\n",
    "![](assets/15.png)\n",
    " \n",
    " \n",
    "3. Output layer gradient\n",
    "=============================\n",
    "![](assets/16.png)\n",
    "![](assets/17.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow3]",
   "language": "python",
   "name": "conda-env-tensorflow3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
